(this.webpackJsonpseungheondoh=this.webpackJsonpseungheondoh||[]).push([[0],{15:function(e){e.exports=JSON.parse('[{"title":"TALKPLAY: Multimodal Music Recommendation with Large Language Models","category":"multimodal conv_sys ret_rec","year":"2025","Authors":"Seungheon Doh, Keunwoo Choi, and Juhan Nam","bookTitle":"ArXiv 2025","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2502.13713","dataset":"https://huggingface.co/datasets/talkpl-ai/talkplay-db-v1","demo":"https://talkpl.ai"}},{"title":"CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages","category":"multimodal ret_rec","year":"2025","Authors":"Shangda Wu, Zhancheng Guo, Ruibin Yuan, Junyan Jiang, Seungheon Doh, Gus Xia, Juhan Nam, Xiaobing Li, Feng Yu, Maosong Sun","bookTitle":"ArXiv 2025","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2502.10362","code":"https://github.com/sanderwood/clamp3"}},{"title":"Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Model","category":"conv_sys ret_rec","year":"2024","Authors":"Seungheon Doh, Keunwoo Choi, Daeyong Kwon, Taesu Kim, and Juhan Nam","bookTitle":"Proceedings of the 25th International Society for Music Information Retrieval Conference (ISMIR), 2024","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2411.07439","dataset":"https://huggingface.co/collections/seungheondoh/lp-musicdialog-6734f28d42d8e0256cb23a1d","demo":"https://ismir24-anonymous.github.io/lp-music-dialogs-demo/"}},{"title":"Enriching Music Descriptions with a Finetuned-LLM and Metadata for Text-to-Music Retrieval","category":"multimodal ret_rec","year":"2024","Authors":"SeungHeon Doh, Minhee Lee, Dasaem Jeong, Juhan Nam","bookTitle":"Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2410.03264","dataset":"https://huggingface.co/collections/seungheondoh/enriching-music-descriptions-661e9342edcea210d61e981d","code":"https://github.com/seungheondoh/music-text-representation-pp","demo":"https://seungheondoh.github.io/music-text-representation-pp-demo/"}},{"title":"LP-MusicCaps: LLM-based Pseudo Music Captioning","category":"multimodal ann","year":"2023","Authors":"Seungheon Doh, Keunwoo Choi, Jongpil Lee, Juhan Nam","bookTitle":"Proceedings of the 24nd International Society for Music Information Retrieval Conference (ISMIR), 2023","bookCategory":"Conferences and Workshops","material":{"pdf":"https://arxiv.org/abs/2307.16372","code":"https://github.com/seungheondoh/lp-music-caps","dataset":"https://huggingface.co/papers/2307.16372","demo":"https://huggingface.co/spaces/seungheondoh/LP-Music-Caps-demo"}}]')},16:function(e){e.exports=JSON.parse('[{"_id":"enrich2024doh","title":"TTMR++: Enriching Music Descriptions with a Finetuned-LLM and Metadata","decs":"This paper proposes an improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes rich text descriptions generated with a finetuned large language model and metadata.","category":"research","url":"https://seungheondoh.github.io/music-text-representation-pp-demo/","year":"2024"},{"_id":"lp2023doh","title":"LP-MusicCaps: LLM-Based Pseudo Music Captioning","decs":"We propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We trained a transformer-based music captioning model.","category":"research","url":"https://github.com/seungheondoh/lp-music-caps/","year":"2023"},{"_id":"toward2023doh","title":"Toward Universal Text-to-Music Retrieval","decs":"This paper introduces effective design choices for text-to-music retrieval systems. An ideal text-based retrieval system would support various input queries such as pre-defined tags, unseen tags, and sentence-level descriptions.","category":"research","url":"https://seungheondoh.github.io/text-music-representation-demo/","year":"2023"},{"_id":"speech2023doh","title":"Speech to Music through Emotion","decs":"Automatic speech emotion recognition (SER) can be used as a music recommendation application for content creators. In this paper, our goal is to help creators find music to match the emotion of their speech.","category":"research","url":"https://seungheondoh.github.io/speech-to-music-demo/","year":"2023"},{"_id":"musical_word_embedding","title":"Musical Word Embedding for Music Annotation and Retrieval","decs":"Musical Word Embedding use a wide spectrum of text corpus from general to music-specific words and defining musical specificity of the corpus as a measure of how the semantics of words is specific to the songs or far from it.","category":"research","url":"https://seungheondoh.github.io/musical_word_embedding_demo/","year":"2022"}]')},25:function(e){e.exports=JSON.parse('[{"id":1,"title":"Seungheon Doh","describtion":"I\'m a Ph.D Student at Music and Audio Computing Lab, working on Music, ML/DL.","detail":"I\'m working on Music and Machine Learning, with a specific focus on advancing our understanding of machines\' ability to perceive music, articulate musical experiences using natural language, and generate visual representations. My primary research efforts are concentrated on advancing the domain of representation learning for music and multimodal media","img":"Seungheon_Doh.jpg","ko_name":"\ub3c4\uc2b9\ud5cc","en_name":"Seungheon Doh","material":{"google scholar":"https://scholar.google.com/citations?user=MCkggcgAAAAJ&hl","github":"https://github.com/seungheondoh","linkedin":"https://www.linkedin.com/in/seungheondoh//","twitter":"https://twitter.com/SeungHeon_Doh"}}]')},26:function(e){e.exports=JSON.parse('[{"date":"Mar-2025","contents":"My LLM4Music proposal has been selected for the KAIST post-doctoral Fellowship. It will receive up to 75,000 USD in funding support.","link":""},{"date":"Feb-2025","contents":"TalkPlay: Multimodal Music Recommendation with Large Language Models is uploaded on Arxiv.","link":"https://arxiv.org/abs/2502.13713"},{"date":"Feb-2025","contents":"I have completed my Ph.D. journey! The title of my doctoral dissertation is \'Connecting Audio and Natural Language for Music Annotation and Retrieval\'. After a year of postdoctoral research (due to military service), I plan to enter the job market next year!.","link":""},{"date":"Nov-2024","contents":"at ISMIR\'24, I will be presenting one tutorial, one paper (LP-MusicDialog), and co-organizing a workshop (NLP4MusA).","link":"https://mulab-mir.github.io/music-language-tutorial/intro.html"},{"date":"May-2024","contents":"I\'m starting a research internship at Adobe. Collaborate with Nicholas J. Bryan and Ge Zhu","link":"https://research.adobe.com/research/audio/"},{"date":"Jan-2024","contents":"I\'m starting a research internship at Chartmetric. Collaborate with Keunwoo Choi","link":"https://chartmetric.com/"},{"date":"Dec-2022","contents":"I\'m starting a research internship at NaverCorp (Now AI Team). Collaborate with Jeong Choi","link":"https://github.com/naver-ai"},{"date":"May-2022","contents":"As a visiting ph.D student, I will visit the NYU Data Science Center. Collaborate with Prof. Kyunghyun Cho.","link":"https://kyunghyuncho.me/"},{"date":"Jul-2021","contents":"I\'m starting a research internship at ByteDance/Tiktok (Speech, Audio, and Music Intelligence Team). Collaborate with Keunwoo Choi, and MinzWon","link":"https://www.bytedance.com/en//"}]')},27:function(e){e.exports=JSON.parse('[{"id":"1","title":"All","filter":"*"},{"id":"2","title":"Multimodality","filter":"multimodal"},{"id":"3","title":"Conversational Systems","filter":"conv_sys"},{"id":"4","title":"Annotation","filter":"ann"},{"id":"3","title":"Retrieval/Recommendation","filter":"ret_rec"},{"id":"4","title":"Generation","filter":"gen"}]')},28:function(e){e.exports=JSON.parse('{"Teaching":[{"date":"Mar-2023","contents":"TA, GCT731 Topics in Music Technology: Generative AI for Music, KAIST GSCT","link":""},{"date":"Sep-2022","contents":"TA, GCT634 Musical Applications of Machine Learning, KAIST GSCT","link":""},{"date":"Sep-2021","contents":"TA, GCT634 Musical Applications of Machine Learning, KAIST GSCT","link":""},{"date":"Sep-2020","contents":"TA, GCT731 Topics in Music Technology: Cognitive Science of Music, KAIST GSCT","link":""},{"date":"Sep-2019","contents":"TA, GCT576 Social Computing, KAIST GSCT","link":""}],"Talk":[{"date":"Feb-2025","contents":"Connecting Music and Large Language Model, Mix.Audio (Host: Jongpil Lee)","link":""},{"date":"Aug-2024","contents":"Connecting Music and Natural Language, TikTok/ByteDance (Host: Ju-Chiang Wang)","link":""},{"date":"Jun-2024","contents":"LLM-Powered Music Annotation and Retrieval, KAIST (Host: Juhan Nam)","link":""},{"date":"Mar-2024","contents":"Text-based Music Annotation and Retrieval, UC San-Diego (Host: Hao-Wen Dong)","link":""},{"date":"Mar-2023","contents":"Multimodal Music Retrieval for Listener and Contents Creator, Seoul Univ (Host: Junghyun Koo)","link":""},{"date":"Dec-2022","contents":"Music Informational Retrieval with Natural Language Processing, YONSEI Univ. (Host: Saebyul Park)","link":""}],"Service":[{"date":"-Current","contents":"Reviewer, IEEE ICASSP, IEEE TASLP, ISMIR, ICML, NeurIPS","link":""},{"date":"Sep-2025","contents":"Organizing Committee, Newcomer Initiative Chair, ISMIR 2025","link":"https://ismir2025.ismir.net/"},{"date":"Nov-2024","contents":"Organizing Committee, NLP4MUSA 2024, 3rd Workshop on NLP for Music and Audio, co-located with ISMIR\'2024","link":"https://sites.google.com/view/nlp4musa-2024/home"},{"date":"Sep-2020","contents":"Korean translator, NYU Deep Learning DS-GA 1008, Yann LeCun & Alfredo Canziani","link":"https://github.com/Atcold/pytorch-Deep-Learning"}]}')},29:function(e){e.exports=JSON.parse('[{"id":"1","school":"Korea Advanced Institute of Science and Technology (KAIST)","position":"Ph.D Student in Graduate School of Culture Technology","duration":"2021 - 2025","advisor":"@ Music and Audio Computing Lab (Advisor: Juhan Nam)"},{"id":"2","school":"Korea Advanced Institute of Science and Technology (KAIST)","position":"MSc. in Graduate School of Culture Technology","duration":"2019 - 2021","advisor":"@ Music and Audio Computing Lab (Advisor: Juhan Nam)","thesis":"Musical Word Embedding for Natural Language based Music Annotation and Retrieval"},{"id":"3","school":"Ulsan National Institute of Science and Technology (UNIST)","position":"B.S. in School of Business administration & Industrial Design","duration":"2014 - 2019"}]')},30:function(e){e.exports=JSON.parse('[{"id":"1","institution":"Adobe Inc.","location":"San Francisco, CA, United States","position":"Research Intern in Music Generation AI Team","duration":"Jun 2024 - Aug 2024","advisor":" (Advisor: Nicholas J. Bryan, Ge Zhu)"},{"id":"1","institution":"Chartmetric","location":"Remote","position":"Research Intern in Audio Analysis Team","duration":"Dec 2023 - Feb 2023","advisor":" (Advisor: Keunwoo Choi)"},{"id":"2","institution":"NaverCorp","location":"1784, South Korea","position":"Research Intern in Now AI Team","duration":"Dec 2022 - Feb 2023","advisor":" (Advisor: Jeong Choi)"},{"id":"3","institution":"Computational Intelligence, Vision, and Robotics Lab (CILVR)","location":"Center for Data Science, New York University, United States","position":"Visiting Student","duration":"Jun 2022 - Aug 2022","advisor":" (Advisor: Kyunghyun Cho)"},{"id":"4","institution":"ByteDance","location":"Remote (due to COVID-19)","position":"Research Intern in Speech, Audio & Music Intelligence Team","duration":"Jul 2021 - Jan 2022","advisor":" (Advisor: Keunwoo Choi, Minz Won)"},{"id":"5","institution":"Music and Audio Research Laboratory (MARL)","location":"Steinhardt, New York University, United States","position":"Visiting Student","duration":"Dec 2019 - Feb 2020","advisor":"(Advisor: TaeHong Park)"}]')},31:function(e){e.exports=JSON.parse('[{"id":"1","title":"All","filter":"*"},{"id":"2","title":"Review","filter":"review"},{"id":"3","title":"Research","filter":"research"}]')},55:function(e,t,i){},56:function(e,t,i){"use strict";i.r(t);var n=i(1),a=i.n(n),o=i(14),s=i.n(o),c=i(32),r=i(2),l=i(8),d=i.n(l),h=i(0);var m=()=>{const e=window.location.href.split("https://seungheondoh.github.io/");return Object(h.jsx)("header",{id:"header",className:"site-header",children:Object(h.jsxs)("div",{className:"wrapper d-flex justify-content-between",children:[Object(h.jsx)("div",{className:"align-self-center",children:Object(h.jsx)("p",{children:"  "})}),Object(h.jsx)("nav",{className:"menu-third",children:Object(h.jsxs)("ul",{className:"clearfix list-unstyled",children:[Object(h.jsx)("li",{className:"menu-item"+("#/"===e[1]?" current-menu-item":""),children:Object(h.jsx)("a",{title:"Home",className:"btn btn-link transform-scale-h border-0 p-0",href:"#/",children:"Home"})}),Object(h.jsx)("li",{className:"menu-item"+("#/blog"===e[1]?" current-menu-item":""),children:Object(h.jsx)("a",{title:"blog",className:"btn btn-link transform-scale-h border-0 p-0",href:"#/blog",children:"Blog"})})]})})]})})};var u=()=>Object(h.jsx)("footer",{id:"footer",className:"site-footer",children:Object(h.jsx)("div",{className:"wrapper no-space",children:Object(h.jsx)("div",{className:"row"})})});var g=e=>{let{keyword:t,link:i,position:n,textcolor:a,backgroundcolor:o}=e;const s=`btn ${n} has-text-color ${a} has-background ${o}`;return Object(h.jsx)("a",{href:i,className:s,children:Object(h.jsx)("b",{children:t})})};var b=e=>{let{keyword:t,link:i,position:n,textcolor:a,backgroundcolor:o}=e;const s=`btn ${n} has-text-color ${a} has-background ${o}`;return Object(h.jsx)("a",{href:i,download:"CV_seunghenodoh",className:s,children:Object(h.jsx)("b",{children:t})})};var j=e=>{let{ProfData:t}=e;return Object(h.jsx)("section",{id:"page-content",className:"spacer",children:Object(h.jsx)("div",{className:"peoplecard",children:Object(h.jsxs)("div",{className:"wrapper",children:[Object(h.jsx)("div",{className:"prof_cardwrapper",children:t.map((e=>Object(h.jsxs)("div",{className:"img_div",children:[Object(h.jsx)("img",{className:"prof_img",src:"/assets/img/people/"+e.img,alt:e.title}),Object(h.jsxs)("div",{className:"info_div",children:[Object(h.jsx)("h4",{children:e.title}),Object(h.jsxs)("p",{className:"p",children:[" I'm a postdoctoral researcher at ",Object(h.jsx)("a",{href:"https://mac.kaist.ac.kr/",children:"Music and Audio Computing Lab"}),", advised by ",Object(h.jsx)("a",{href:"https://mac.kaist.ac.kr/~juhan/",children:"Prof. Juhan Nam"}),". ",Object(h.jsx)("br",{}),"My research focuses on the machine's ability to listen to music, express music experience in natural language, and imagine visuals. A key aspect of my research lies in representation learning, particularly in bridging the gap between music and multi-modal media. Presently, my primary focus is on ",Object(h.jsx)("b",{children:"multi-turn conversation"})," and ",Object(h.jsx)("b",{children:"multi-modality"}),". I aim for machines to comprehend diverse modalities during conversations, particularly in the context of music generation and retrieval, facilitating the discovery of music through dialogue."]}),Object(h.jsxs)("div",{className:"btn_div",children:[Object(h.jsx)(b,{keyword:"cv",link:"/assets/cv/CV_seungheon.pdf",position:"",textcolor:"has-white-color",backgroundcolor:"has-olive-background-color"}),Object.keys(e.material).map(((t,i)=>Object(h.jsx)(g,{keyword:t,link:e.material[t],position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})))]})]})]})))}),Object(h.jsxs)("p",{className:"p",style:{textAlign:"center",marginTop:"30px",marginBottom:"5px"},children:[" ",Object(h.jsx)("b",{children:"\ud83d\udd25 Plan to enter the job market at 2026 March! \ud83d\udd25"})]})]})})})},p=i(25);var x=e=>{let{NewsInfoData:t}=e;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-top-lg p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"news wrapper",children:[Object(h.jsx)("h4",{children:"News"}),Object(h.jsx)("ul",{children:t.map((e=>""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("li",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("li",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})})))}),Object(h.jsx)("hr",{})]})})})},O=i(26),v=i(12),f=i.n(v),k=i(27),w=i(15);class y extends n.Component{constructor(e){super(e),this.onFilterChange=e=>{const t=this.grid;void 0===this.iso&&(this.iso=new f.a(t,{itemSelector:".publicationTable-item",masonry:{horizontalOrder:!0}})),"*"===e?this.iso.arrange({filter:"*"}):this.iso.arrange({filter:`.${e}`})},this.onFilterChange=this.onFilterChange.bind(this),this.state={selected:0,list:[...k]}}handleClick(e,t){return t.preventDefault(),this.setState({selected:e}),!1}componentDidMount(){}render(){const e=this.state.list.length-1;return Object(h.jsx)("div",{className:"publicationTable spacer p-bottom-lg",children:Object(h.jsxs)("div",{className:"wrapper",children:[Object(h.jsx)("h4",{children:"Selected Publications"}),Object(h.jsx)("ul",{className:"publicationTable-filter",children:this.state.list.map(((t,i)=>Object(h.jsxs)(a.a.Fragment,{children:[Object(h.jsx)("li",{children:Object(h.jsx)("span",{title:t.title,className:"btn btn-link transform-scale-h click"+(i===this.state.selected?" active":""),"data-filter":t.filter,onClick:e=>{this.onFilterChange(t.filter),this.handleClick(i,e)},children:t.title})}),i!==e?Object(h.jsx)("li",{children:Object(h.jsx)("span",{className:"btn btn-link",children:"-"})}):""]},i)))}),Object(h.jsxs)("div",{className:"publicationTable-item-wrapper",children:[Object(h.jsx)("div",{className:"publicationTable-items",ref:e=>this.grid=e,children:w&&w.map(((e,t)=>""===e.material?Object(h.jsxs)("div",{title:e.title,className:"publicationTable-item active "+e.category,children:[Object(h.jsx)("h6",{children:e.title}),Object(h.jsx)("p",{className:"no-line-hight",children:e.Authors}),Object(h.jsx)("p",{className:"date",children:e.bookTitle})]},t):Object(h.jsxs)("div",{title:e.title,className:"publicationTable-item active "+e.category,children:[Object(h.jsx)("h6",{children:e.title}),Object(h.jsx)("p",{className:"no-line-hight",children:e.Authors}),Object(h.jsx)("p",{className:"date",children:e.bookTitle}),Object.keys(e.material).map(((t,i)=>0===i?Object(h.jsx)(g,{keyword:t,link:e.material[t],position:"",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"}):Object(h.jsx)(g,{keyword:t,link:e.material[t],position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})))]},t)))}),Object(h.jsxs)("h6",{className:"margin",children:["More Publication @ ",Object(h.jsx)("a",{className:"active-color",href:"https://scholar.google.com/citations?user=MCkggcgAAAAJ&hl=en",children:" Google Scholar"})," Page"]})]})]})})}}var M=y;var N=e=>{let{ExpInfoData:t}=e;const i=t.Service,n=t.Talk,a=t.Teaching;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"wrapper",children:[Object(h.jsx)("h4",{children:"Service, Talk, Teaching"}),Object(h.jsxs)("div",{className:"experience",children:[Object(h.jsx)("h6",{children:"Service"}),i.map((e=>""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})}))),Object(h.jsx)("h6",{children:"Talk"}),n.map((e=>""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})}))),Object(h.jsx)("h6",{children:"Teaching"}),a.map((e=>""===e.link?Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents]})}):Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)("p",{children:[" ",e.date," | ",e.contents,Object(h.jsx)(g,{keyword:"Link",link:e.link,position:"inline",textcolor:"has-white-color",backgroundcolor:"has-gray-dark-background-color"})]})})))]})]})})})},S=i(28);var C=e=>{let{EduInfoData:t}=e;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"eduacation wrapper",children:[Object(h.jsx)("h4",{children:"Education"}),Object(h.jsx)("div",{className:"eduacation",children:t.map((e=>Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("h6",{children:e.school}),Object(h.jsxs)("p",{children:[e.position," ",""!==e.advisor?Object(h.jsx)("p",{children:e.advisor}):null]}),Object(h.jsx)("p",{className:"date",children:e.duration})]})))})]})})})},A=i(29);var T=e=>{let{IndInfoData:t}=e;return Object(h.jsx)("section",{id:"page-content",className:"spacer p-bottom-lg",children:Object(h.jsx)("div",{id:"blog",children:Object(h.jsxs)("div",{className:"industry wrapper",children:[Object(h.jsx)("h4",{children:"Experience"}),Object(h.jsx)("div",{className:"eduacation",children:t.map((e=>Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("h6",{children:e.institution}),Object(h.jsxs)("p",{children:[e.position," ",""!==e.advisor?Object(h.jsx)("span",{children:e.advisor}):null]}),Object(h.jsxs)("p",{className:"date",children:[e.location," | ",e.duration]})]})))})]})})})},I=i(30);var L=()=>(document.body.classList.add("home"),document.body.classList.add("bg-fixed"),document.body.classList.add("bg-line"),Object(h.jsxs)(n.Fragment,{children:[Object(h.jsxs)(d.a,{children:[Object(h.jsx)("meta",{charSet:"UTF-8"}),Object(h.jsx)("title",{children:"SeungHeon Doh | MIR, ML/DL Researcher"}),Object(h.jsx)("link",{rel:"icon",href:"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>\ud83c\udfb9</text></svg>"}),Object(h.jsx)("meta",{httpEquiv:"x-ua-compatible",content:"ie=edge"}),Object(h.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"}),Object(h.jsx)("meta",{name:"description",content:"Music informational retrieval, Machine learning, Deep learning"}),Object(h.jsx)("meta",{name:"keywords",content:"music information retrieval, MIR, machine learning, ML, deep learning, DL, SeungHeon Doh"}),Object(h.jsx)("meta",{name:"robots",content:"index, follow"}),Object(h.jsx)("meta",{name:"googlebot",content:"index, follow"}),Object(h.jsx)("meta",{name:"google",content:"notranslate"}),Object(h.jsx)("meta",{name:"format-detection",content:"telephone=no"})]}),Object(h.jsx)(m,{}),Object(h.jsxs)("main",{id:"main",className:"site-main",children:[Object(h.jsx)(j,{ProfData:p}),Object(h.jsx)(x,{NewsInfoData:O}),Object(h.jsx)(M,{}),Object(h.jsx)(T,{IndInfoData:I}),Object(h.jsx)(C,{EduInfoData:A}),Object(h.jsx)(N,{ExpInfoData:S})]}),Object(h.jsx)(u,{})]})),D=i(31),R=i(16);var J=e=>{let{item:t,key:i}=e;return Object(h.jsx)("section",{title:t.title,className:"blogTable-item active "+t.category,children:Object(h.jsxs)("div",{className:"contents_div",children:[Object(h.jsx)("a",{href:t.url,cursor:"pointer",children:Object(h.jsx)("img",{className:"img_div",src:"/assets/blog/"+t._id+"/thumbnail.png",alt:t._id})}),Object(h.jsxs)("div",{className:"info_div",children:[Object(h.jsxs)("div",{children:[Object(h.jsx)("h5",{children:Object(h.jsx)("a",{href:t.url,cursor:"pointer",children:t.title})}),Object(h.jsx)("p",{className:"desc",children:t.decs})]}),Object(h.jsx)("div",{className:"date_div",children:Object(h.jsx)("p",{className:"blogdate",children:t.year})})]})]})},i)};class F extends n.Component{constructor(e){super(e),this.onFilterChange=e=>{const t=this.grid;void 0===this.iso&&(this.iso=new f.a(t,{itemSelector:".blogTable-item",masonry:{horizontalOrder:!0}})),"*"===e?this.iso.arrange({filter:"*"}):this.iso.arrange({filter:`.${e}`})},this.onFilterChange=this.onFilterChange.bind(this),this.state={selected:0,list:D}}handleClick(e,t){return t.preventDefault(),this.setState({selected:e}),!1}componentDidMount(){}render(){const e=this.state.list.length-1;return Object(h.jsx)("div",{className:"blogTable spacer p-bottom-lg",children:Object(h.jsxs)("div",{className:"blogTable-wrapper",children:[Object(h.jsx)("h3",{children:"Blogs"}),Object(h.jsx)("ul",{className:"blogTable-filter",children:this.state.list.map(((t,i)=>Object(h.jsxs)(a.a.Fragment,{children:[Object(h.jsx)("li",{children:Object(h.jsx)("span",{title:t.title,className:"btn btn-link transform-scale-h click"+(i===this.state.selected?" active":""),"data-filter":t.filter,onClick:e=>{this.onFilterChange(t.filter),this.handleClick(i,e)},children:t.title})}),i!==e?Object(h.jsx)("li",{children:Object(h.jsx)("span",{className:"btn btn-link",children:"-"})}):""]},i)))}),Object(h.jsx)("div",{className:"blogTable-item-wrapper",children:Object(h.jsx)("div",{className:"blogTable-items",ref:e=>this.grid=e,children:R&&R.map(((e,t)=>(e.material,Object(h.jsx)(J,{item:e},t))))})})]})})}}var P=F;var _=()=>(document.body.classList.add("blog"),document.body.classList.add("bg-fixed"),document.body.classList.add("bg-line"),Object(h.jsxs)(n.Fragment,{children:[Object(h.jsxs)(d.a,{children:[Object(h.jsx)("meta",{charSet:"UTF-8"}),Object(h.jsx)("title",{children:"SeungHeon Doh | MIR Researcher, ML/DL Engineer"}),Object(h.jsx)("link",{rel:"icon",href:"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>\ud83c\udfb9</text></svg>"}),Object(h.jsx)("meta",{httpEquiv:"x-ua-compatible",content:"ie=edge"}),Object(h.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1"}),Object(h.jsx)("meta",{name:"description",content:""}),Object(h.jsx)("meta",{name:"keywords",content:""}),Object(h.jsx)("meta",{name:"robots",content:"index, follow, noodp"}),Object(h.jsx)("meta",{name:"googlebot",content:"index, follow"}),Object(h.jsx)("meta",{name:"google",content:"notranslate"}),Object(h.jsx)("meta",{name:"format-detection",content:"telephone=no"})]}),Object(h.jsx)(m,{}),Object(h.jsx)("main",{id:"main",className:"site-main",children:Object(h.jsx)("div",{className:"wrapper",children:Object(h.jsx)(P,{})})}),Object(h.jsx)(u,{})]}));var E=()=>(document.body.classList.add("thesis"),document.body.classList.add("bg-fixed"),document.body.classList.add("bg-line"),Object(h.jsxs)(n.Fragment,{children:[Object(h.jsxs)(d.a,{children:[Object(h.jsx)("meta",{charSet:"UTF-8"}),Object(h.jsx)("title",{children:"SeungHeon Doh | MIR, ML/DL Researcher"}),Object(h.jsx)("link",{rel:"icon",href:"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>\ud83c\udfb9</text></svg>"}),Object(h.jsx)("meta",{httpEquiv:"x-ua-compatible",content:"ie=edge"}),Object(h.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1"}),Object(h.jsx)("meta",{name:"description",content:""}),Object(h.jsx)("meta",{name:"keywords",content:""}),Object(h.jsx)("meta",{name:"robots",content:"index, follow, noodp"}),Object(h.jsx)("meta",{name:"googlebot",content:"index, follow"}),Object(h.jsx)("meta",{name:"google",content:"notranslate"}),Object(h.jsx)("meta",{name:"format-detection",content:"telephone=no"})]}),Object(h.jsx)(m,{}),Object(h.jsx)("main",{id:"main",className:"site-main",children:Object(h.jsx)("div",{className:"wrapper",children:Object(h.jsx)("p",{children:"Comming Soon"})})}),Object(h.jsx)(u,{})]}));var K=function(){return Object(h.jsxs)(c.a,{basename:"",children:[Object(h.jsx)(r.b,{exact:!0,path:"/",component:L}),Object(h.jsx)(r.b,{path:"/blog",component:_}),Object(h.jsx)(r.b,{path:"/thesis",component:E}),Object(h.jsx)(r.a,{to:"/"})]})};i(55);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));s.a.render(Object(h.jsx)(K,{}),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((e=>{e.unregister()})).catch((e=>{console.error(e.message)}))}},[[56,1,2]]]);
//# sourceMappingURL=main.dd1bb7e1.chunk.js.map